{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d483ef3-f76b-4d98-a343-5e880bb53b74",
   "metadata": {},
   "source": [
    "# Implementation of Single Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e41d5e-7269-4a47-8c7b-7b41aad3d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926996bf-ae3b-40e7-bef9-2d03728c7b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([2.0,3.0,4.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b81149f4-2599-4f54-a274-e0b39578c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([0.5,-1.2,2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7e25ca-9b10-463d-b23a-baa9bbc05162",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e78a42e9-5689-49a8-9642-eb440c74cfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sum = np.dot(inputs,weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b154d46-a830-40ce-b034-489ebb3f989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38a920da-efa6-484c-926f-1c53320f2651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron Output : 6.1000000000000005\n"
     ]
    }
   ],
   "source": [
    "output = relu(weighted_sum)\n",
    "print(\"Neuron Output :\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616bb71a-7742-4b23-9883-55a45c1c7f08",
   "metadata": {},
   "source": [
    "ðŸ“Œ Formula of a Neuron\n",
    "y=f(W1x1+W2x2+...+Wnxn+b)\n",
    "y=f(W1â€‹x1â€‹+W2â€‹x2â€‹+...+Wnâ€‹xnâ€‹+b)\n",
    "\n",
    "Where:\n",
    "\n",
    "    xx = Inputs\n",
    "    WW = Weights\n",
    "    bb = Bias\n",
    "    ff = Activation function\n",
    "    yy = Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1898d9c-6fd8-4b7f-baad-ce4a556a3493",
   "metadata": {},
   "source": [
    "# Implementing simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cd41565-b426-448c-a265-b673ee4e6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbf2db5d-02ce-4a52-864e-3c38a8641e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "  return x*(1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67dd5eec-e2e5-473c-8e25-3ff4e823218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([\n",
    "    [0, 0], \n",
    "    [0, 1], \n",
    "    [1, 0], \n",
    "    [1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "488f643d-57ed-4490-870f-d532254f5c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = np.array([[0], [1], [1], [0]])  # XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14b1b91d-1200-4893-9f05-4fe4547fe1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "weights_input_hidden = np.random.rand(2,2)\n",
    "bias_hidden = np.random.rand(2)\n",
    "\n",
    "weights_hidden_output = np.random.rand(2,2)\n",
    "bias_output = np.random.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b91a0f1-9d00-4b16-920e-b0ce8176475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc4b9f4f-b68f-4598-867e-9931a4ac5a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , Loss : 0.0018575400077845473\n",
      "Epoch 1000 , Loss : 0.0016170039481699585\n",
      "Epoch 2000 , Loss : 0.0014301854249653611\n",
      "Epoch 3000 , Loss : 0.0012810870464378937\n",
      "Epoch 4000 , Loss : 0.0011594512715384852\n",
      "Epoch 5000 , Loss : 0.0010584082033913314\n",
      "Epoch 6000 , Loss : 0.000973189271741769\n",
      "Epoch 7000 , Loss : 0.0009003840288585099\n",
      "Epoch 8000 , Loss : 0.0008374906639351765\n",
      "Epoch 9000 , Loss : 0.0007826334577581751\n",
      "Epoch 10000 , Loss : 0.0007343792068349568\n",
      "Epoch 11000 , Loss : 0.0006916144858057336\n",
      "Epoch 12000 , Loss : 0.0006534615045279837\n",
      "Epoch 13000 , Loss : 0.0006192191347194869\n",
      "Epoch 14000 , Loss : 0.0005883207570741994\n",
      "Epoch 15000 , Loss : 0.0005603035967863608\n",
      "Epoch 16000 , Loss : 0.0005347860604653003\n",
      "Epoch 17000 , Loss : 0.0005114507448945767\n",
      "Epoch 18000 , Loss : 0.0004900315310782989\n",
      "Epoch 19000 , Loss : 0.0004703036639618389\n",
      "Epoch 20000 , Loss : 0.0004520760434415545\n",
      "Epoch 21000 , Loss : 0.00043518517327850617\n",
      "Epoch 22000 , Loss : 0.00041949036710634956\n",
      "Epoch 23000 , Loss : 0.00040486991760613623\n",
      "Epoch 24000 , Loss : 0.0003912180108107652\n",
      "Epoch 25000 , Loss : 0.0003784422220632831\n",
      "Epoch 26000 , Loss : 0.00036646146983825326\n",
      "Epoch 27000 , Loss : 0.00035520433281417425\n",
      "Epoch 28000 , Loss : 0.00034460765725627006\n",
      "Epoch 29000 , Loss : 0.0003346153980179641\n",
      "Epoch 30000 , Loss : 0.000325177648760767\n",
      "Epoch 31000 , Loss : 0.0003162498263682307\n",
      "Epoch 32000 , Loss : 0.00030779198173790584\n",
      "Epoch 33000 , Loss : 0.0002997682147182611\n",
      "Epoch 34000 , Loss : 0.0002921461753120361\n",
      "Epoch 35000 , Loss : 0.00028489663668666685\n",
      "Epoch 36000 , Loss : 0.0002779931282337278\n",
      "Epoch 37000 , Loss : 0.0002714116190666551\n",
      "Epoch 38000 , Loss : 0.0002651302440622966\n",
      "Epoch 39000 , Loss : 0.00025912906593132255\n",
      "Epoch 40000 , Loss : 0.00025338986791688444\n",
      "Epoch 41000 , Loss : 0.00024789597262547916\n",
      "Epoch 42000 , Loss : 0.0002426320832318848\n",
      "Epoch 43000 , Loss : 0.00023758414390444475\n",
      "Epoch 44000 , Loss : 0.00023273921679442555\n",
      "Epoch 45000 , Loss : 0.0002280853733440082\n",
      "Epoch 46000 , Loss : 0.00022361159800840503\n",
      "Epoch 47000 , Loss : 0.0002193077027712628\n",
      "Epoch 48000 , Loss : 0.0002151642510697237\n",
      "Epoch 49000 , Loss : 0.00021117248994430906\n",
      "Epoch 50000 , Loss : 0.00020732428939606201\n",
      "Epoch 51000 , Loss : 0.00020361208807462915\n",
      "Epoch 52000 , Loss : 0.00020002884454051848\n",
      "Epoch 53000 , Loss : 0.00019656799344623205\n",
      "Epoch 54000 , Loss : 0.00019322340606753676\n",
      "Epoch 55000 , Loss : 0.0001899893546899441\n",
      "Epoch 56000 , Loss : 0.0001868604804187268\n",
      "Epoch 57000 , Loss : 0.00018383176403508902\n",
      "Epoch 58000 , Loss : 0.00018089849956793197\n",
      "Epoch 59000 , Loss : 0.00017805627029084147\n",
      "Epoch 60000 , Loss : 0.00017530092688901533\n",
      "Epoch 61000 , Loss : 0.00017262856757092039\n",
      "Epoch 62000 , Loss : 0.00017003551992582025\n",
      "Epoch 63000 , Loss : 0.00016751832435121268\n",
      "Epoch 64000 , Loss : 0.0001650737188941319\n",
      "Epoch 65000 , Loss : 0.00016269862536777638\n",
      "Epoch 66000 , Loss : 0.00016039013662015714\n",
      "Epoch 67000 , Loss : 0.00015814550484492026\n",
      "Epoch 68000 , Loss : 0.0001559621308362929\n",
      "Epoch 69000 , Loss : 0.00015383755410046265\n",
      "Epoch 70000 , Loss : 0.00015176944374492243\n",
      "Epoch 71000 , Loss : 0.0001497555900753889\n",
      "Epoch 72000 , Loss : 0.0001477938968370588\n",
      "Epoch 73000 , Loss : 0.00014588237404339948\n",
      "Epoch 74000 , Loss : 0.00014401913134125033\n",
      "Epoch 75000 , Loss : 0.00014220237186609246\n",
      "Epoch 76000 , Loss : 0.00014043038654580847\n",
      "Epoch 77000 , Loss : 0.00013870154881521834\n",
      "Epoch 78000 , Loss : 0.00013701430970735042\n",
      "Epoch 79000 , Loss : 0.00013536719329044458\n",
      "Epoch 80000 , Loss : 0.00013375879242269679\n",
      "Epoch 81000 , Loss : 0.00013218776479926995\n",
      "Epoch 82000 , Loss : 0.0001306528292683253\n",
      "Epoch 83000 , Loss : 0.00012915276239506163\n",
      "Epoch 84000 , Loss : 0.0001276863952544524\n",
      "Epoch 85000 , Loss : 0.0001262526104351998\n",
      "Epoch 86000 , Loss : 0.00012485033923881808\n",
      "Epoch 87000 , Loss : 0.00012347855905923472\n",
      "Epoch 88000 , Loss : 0.0001221362909294834\n",
      "Epoch 89000 , Loss : 0.00012082259722319134\n",
      "Epoch 90000 , Loss : 0.0001195365794996305\n",
      "Epoch 91000 , Loss : 0.00011827737648195093\n",
      "Epoch 92000 , Loss : 0.00011704416215914194\n",
      "Epoch 93000 , Loss : 0.00011583614400294598\n",
      "Epoch 94000 , Loss : 0.00011465256129173664\n",
      "Epoch 95000 , Loss : 0.00011349268353391894\n",
      "Epoch 96000 , Loss : 0.000112355808984064\n",
      "Epoch 97000 , Loss : 0.00011124126324545534\n",
      "Epoch 98000 , Loss : 0.00011014839795326904\n",
      "Epoch 99000 , Loss : 0.00010907658953298514\n",
      "\n",
      " Final output after training:\n",
      "[[0.01106737 0.01106717]\n",
      " [0.98989748 0.98989222]\n",
      " [0.98989518 0.98988993]\n",
      " [0.01026012 0.01026754]]\n"
     ]
    }
   ],
   "source": [
    "epochs = 100000\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # forward propagation\n",
    "    hidden_input = np.dot(inputs,weights_input_hidden) + bias_hidden\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "    final_input = np.dot(hidden_output,weights_hidden_output) + bias_output\n",
    "    final_output = sigmoid(final_input)\n",
    "\n",
    "    #compute Loss\n",
    "    loss = np.mean((outputs - final_output) ** 2)\n",
    "\n",
    "    #Backpropagation\n",
    "    error = outputs - final_output\n",
    "    d_output = error * sigmoid_derivative(final_output)\n",
    "\n",
    "    error_hidden = d_output.dot(weights_hidden_output.T)\n",
    "    d_hidden = error_hidden * sigmoid_derivative(hidden_output)\n",
    "\n",
    "    # update weights & biases\n",
    "    weights_hidden_output += hidden_output.T.dot(d_output) * alpha\n",
    "    bias_output += np.sum(d_output) * alpha\n",
    "\n",
    "    weights_input_hidden += inputs.T.dot(d_hidden) * alpha\n",
    "    bias_hidden += np.sum(d_hidden,axis = 0) * alpha\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch} , Loss : {loss}\")\n",
    "\n",
    "#final\n",
    "\n",
    "print(\"\\n Final output after training:\")\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae0b6f-ea87-41c9-bf2f-b39f7d64dd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Correct)",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
